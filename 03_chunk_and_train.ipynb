{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "034110d7",
   "metadata": {},
   "source": [
    "## In this notebook, we attempted to finetune two large language models (miniLM, RoBERTa) to classify transcript chunks as right wing or left wing. \n",
    "\n",
    "- Author: Bowen Yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774bef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "device = 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= str(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a1bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d34e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"07_Data_annotated_transcript.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a72c7ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_to_chunk</th>\n",
       "      <th>path</th>\n",
       "      <th>url</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>Annotator Assigned</th>\n",
       "      <th>Human Label</th>\n",
       "      <th>transcript_to_annotate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['But I am not going to sit here and do that. ...</td>\n",
       "      <td>/api.substack.com/ef/httpsapi.substack.comfeed...</td>\n",
       "      <td>https://api.substack.com/feed/podcast/80790471...</td>\n",
       "      <td>news</td>\n",
       "      <td>politics</td>\n",
       "      <td>commentary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alan</td>\n",
       "      <td>0</td>\n",
       "      <td>But I am not going to sit here and do that. An...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 transcript_to_chunk  \\\n",
       "0  ['But I am not going to sit here and do that. ...   \n",
       "\n",
       "                                                path  \\\n",
       "0  /api.substack.com/ef/httpsapi.substack.comfeed...   \n",
       "\n",
       "                                                 url  cat1      cat2  \\\n",
       "0  https://api.substack.com/feed/podcast/80790471...  news  politics   \n",
       "\n",
       "         cat3 cat4 cat5 cat6 cat7  cat8  cat9  cat10 Annotator Assigned  \\\n",
       "0  commentary  NaN  NaN  NaN  NaN   NaN   NaN    NaN               Alan   \n",
       "\n",
       "   Human Label                             transcript_to_annotate  \n",
       "0            0  But I am not going to sit here and do that. An...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a93606b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Human Label']==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9cd10be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Human Label']==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c1fd106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['transcript_to_chunk'].isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6b1ac0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(879, 16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[~df['transcript_to_chunk'].isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fb1064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['transcript_to_chunk'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40c001c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(879, 16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62541ff7",
   "metadata": {},
   "source": [
    "## 1. Split annotated transcripts to train, dev, and test sets (7:1:2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff355cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_dev_test = train_test_split(df, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "462a1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev, df_test = train_test_split(df_dev_test, test_size=2/3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91634cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57e45e5e",
   "metadata": {},
   "source": [
    "## 2. Chunk transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40a82b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_transcript(text, chunk_size=100, max_size=256):\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    current_words_count = 0 \n",
    "\n",
    "    for sentence in text:\n",
    "        words_in_sentence = len(sentence.split())\n",
    "        \n",
    "        if current_words_count + words_in_sentence > chunk_size:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "            current_words_count = words_in_sentence\n",
    "        else:\n",
    "            current_chunk += sentence\n",
    "            current_words_count += words_in_sentence\n",
    "\n",
    "    if current_chunk.strip():\n",
    "        if chunks and len(chunks[-1].split()) + len(current_chunk.split()) <= max_size:\n",
    "               chunks[-1] += '' + current_chunk\n",
    "        else:\n",
    "               chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93f69de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transcript_to_chunk', 'path', 'url', 'cat1', 'cat2', 'cat3', 'cat4',\n",
       "       'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'Annotator Assigned',\n",
       "       'Human Label', 'transcript_to_annotate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f843aad2",
   "metadata": {},
   "source": [
    "### 2.1 Chunk train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56dd368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "train_labels = []\n",
    "train_paths = []\n",
    "train_chunk_ind = []\n",
    "train_cat1 = []\n",
    "train_cat2 = []\n",
    "train_cat3 = []\n",
    "train_cat4 = []\n",
    "train_cat5 = []\n",
    "train_cat6 = []\n",
    "train_cat7 = []\n",
    "train_cat8 = []\n",
    "train_cat9 = []\n",
    "train_cat10 = []\n",
    "\n",
    "for _, row in df_train.iterrows():\n",
    "    chunks = chunk_transcript(row['transcript_to_chunk'])\n",
    "    chunk_count = len(chunks)\n",
    "    \n",
    "    if chunks:\n",
    "        train_texts.extend(chunks)\n",
    "        train_labels.extend([row[\"Human Label\"]] * chunk_count)\n",
    "        train_paths.extend([row[\"path\"]] * chunk_count)\n",
    "\n",
    "        \n",
    "        chunk_ind = list(range(1, chunk_count+1))\n",
    "        train_chunk_ind.extend(chunk_ind)\n",
    "\n",
    "        train_cat1.extend([row[\"cat1\"]] * chunk_count)\n",
    "        train_cat2.extend([row[\"cat2\"]] * chunk_count)\n",
    "        train_cat3.extend([row[\"cat3\"]] * chunk_count)\n",
    "        train_cat4.extend([row[\"cat4\"]] * chunk_count)\n",
    "        train_cat5.extend([row[\"cat5\"]] * chunk_count)\n",
    "        train_cat6.extend([row[\"cat6\"]] * chunk_count)\n",
    "        train_cat7.extend([row[\"cat7\"]] * chunk_count)\n",
    "        train_cat8.extend([row[\"cat8\"]] * chunk_count)\n",
    "        train_cat9.extend([row[\"cat9\"]] * chunk_count)\n",
    "        train_cat10.extend([row[\"cat10\"]] * chunk_count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e352bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "    'text':train_texts,\n",
    "    'label':train_labels,\n",
    "    'path':train_paths,\n",
    "    'chunk_pos':train_chunk_ind,\n",
    "    'cat1':train_cat1,\n",
    "    'cat2':train_cat2,\n",
    "    'cat3':train_cat3,\n",
    "    'cat4':train_cat4,\n",
    "    'cat5':train_cat5,\n",
    "    'cat6':train_cat6,\n",
    "    'cat7':train_cat7,\n",
    "    'cat8':train_cat8,\n",
    "    'cat9':train_cat9,\n",
    "    'cat10':train_cat10,\n",
    "}\n",
    "\n",
    "df_chunks_train = pd.DataFrame(train_data)\n",
    "df_chunks_train = df_chunks_train[df_chunks_train['text'].str.strip() != '']\n",
    "df_chunks_train = df_chunks_train.reset_index(drop=True)\n",
    "df_chunks_train = df_chunks_train.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12013d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124553, 14)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0027a280",
   "metadata": {},
   "source": [
    "### 2.2 Chunk dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9d878cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_texts = []\n",
    "dev_labels = []\n",
    "dev_paths = []\n",
    "dev_chunk_ind = []\n",
    "dev_cat1 = []\n",
    "dev_cat2 = []\n",
    "dev_cat3 = []\n",
    "dev_cat4 = []\n",
    "dev_cat5 = []\n",
    "dev_cat6 = []\n",
    "dev_cat7 = []\n",
    "dev_cat8 = []\n",
    "dev_cat9 = []\n",
    "dev_cat10 = []\n",
    "\n",
    "for _, row in df_dev.iterrows():\n",
    "    chunks = chunk_transcript(row['transcript_to_chunk'])\n",
    "    chunk_count = len(chunks)\n",
    "    \n",
    "    if chunks:\n",
    "        dev_texts.extend(chunks)\n",
    "        dev_labels.extend([row[\"Human Label\"]] * chunk_count)\n",
    "        dev_paths.extend([row[\"path\"]] * chunk_count)\n",
    "        \n",
    "        chunk_ind = list(range(1, chunk_count+1))\n",
    "        dev_chunk_ind.extend(chunk_ind)\n",
    "\n",
    "        dev_cat1.extend([row[\"cat1\"]] * chunk_count)\n",
    "        dev_cat2.extend([row[\"cat2\"]] * chunk_count)\n",
    "        dev_cat3.extend([row[\"cat3\"]] * chunk_count)\n",
    "        dev_cat4.extend([row[\"cat4\"]] * chunk_count)\n",
    "        dev_cat5.extend([row[\"cat5\"]] * chunk_count)\n",
    "        dev_cat6.extend([row[\"cat6\"]] * chunk_count)\n",
    "        dev_cat7.extend([row[\"cat7\"]] * chunk_count)\n",
    "        dev_cat8.extend([row[\"cat8\"]] * chunk_count)\n",
    "        dev_cat9.extend([row[\"cat9\"]] * chunk_count)\n",
    "        dev_cat10.extend([row[\"cat10\"]] * chunk_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7bd13b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = {\n",
    "    'text':dev_texts,\n",
    "    'label':dev_labels,\n",
    "    'path':dev_paths,\n",
    "    'chunk_pos':dev_chunk_ind,\n",
    "    'cat1':dev_cat1,\n",
    "    'cat2':dev_cat2,\n",
    "    'cat3':dev_cat3,\n",
    "    'cat4':dev_cat4,\n",
    "    'cat5':dev_cat5,\n",
    "    'cat6':dev_cat6,\n",
    "    'cat7':dev_cat7,\n",
    "    'cat8':dev_cat8,\n",
    "    'cat9':dev_cat9,\n",
    "    'cat10':dev_cat10,\n",
    "}\n",
    "\n",
    "df_chunks_dev = pd.DataFrame(dev_data)\n",
    "df_chunks_dev = df_chunks_dev[df_chunks_dev['text'].str.strip() != '']\n",
    "df_chunks_dev = df_chunks_dev.reset_index(drop=True)\n",
    "df_chunks_dev = df_chunks_dev.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1debe90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16789, 14)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70efb3",
   "metadata": {},
   "source": [
    "### 2.3 Chunk test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da451b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = []\n",
    "test_labels = []\n",
    "test_paths = []\n",
    "test_chunk_ind = []\n",
    "test_cat1 = []\n",
    "test_cat2 = []\n",
    "test_cat3 = []\n",
    "test_cat4 = []\n",
    "test_cat5 = []\n",
    "test_cat6 = []\n",
    "test_cat7 = []\n",
    "test_cat8 = []\n",
    "test_cat9 = []\n",
    "test_cat10 = []\n",
    "\n",
    "for _, row in df_test.iterrows():\n",
    "    chunks = chunk_transcript(row['transcript_to_chunk'])\n",
    "    chunk_count = len(chunks)\n",
    "    \n",
    "    if chunks:\n",
    "        test_texts.extend(chunks)\n",
    "        test_labels.extend([row[\"Human Label\"]] * chunk_count)\n",
    "        test_paths.extend([row[\"path\"]] * chunk_count)\n",
    "        \n",
    "        chunk_ind = list(range(1, chunk_count+1))\n",
    "        test_chunk_ind.extend(chunk_ind)\n",
    "\n",
    "        test_cat1.extend([row[\"cat1\"]] * chunk_count)\n",
    "        test_cat2.extend([row[\"cat2\"]] * chunk_count)\n",
    "        test_cat3.extend([row[\"cat3\"]] * chunk_count)\n",
    "        test_cat4.extend([row[\"cat4\"]] * chunk_count)\n",
    "        test_cat5.extend([row[\"cat5\"]] * chunk_count)\n",
    "        test_cat6.extend([row[\"cat6\"]] * chunk_count)\n",
    "        test_cat7.extend([row[\"cat7\"]] * chunk_count)\n",
    "        test_cat8.extend([row[\"cat8\"]] * chunk_count)\n",
    "        test_cat9.extend([row[\"cat9\"]] * chunk_count)\n",
    "        test_cat10.extend([row[\"cat10\"]] * chunk_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "520c76dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {\n",
    "    'text':test_texts,\n",
    "    'label':test_labels,\n",
    "    'path':test_paths,\n",
    "    'chunk_pos':test_chunk_ind,\n",
    "    'cat1':test_cat1,\n",
    "    'cat2':test_cat2,\n",
    "    'cat3':test_cat3,\n",
    "    'cat4':test_cat4,\n",
    "    'cat5':test_cat5,\n",
    "    'cat6':test_cat6,\n",
    "    'cat7':test_cat7,\n",
    "    'cat8':test_cat8,\n",
    "    'cat9':test_cat9,\n",
    "    'cat10':test_cat10,\n",
    "}\n",
    "\n",
    "df_chunks_test = pd.DataFrame(test_data)\n",
    "df_chunks_test = df_chunks_test[df_chunks_test['text'].str.strip() != '']\n",
    "df_chunks_test = df_chunks_test.reset_index(drop=True)\n",
    "df_chunks_test = df_chunks_test.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc1cd136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33690, 14)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "90e4cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks_test.to_csv(\"df_chunks_test.csv\", index=False)\n",
    "df_chunks_dev.to_csv(\"df_chunks_dev.csv\", index=False)\n",
    "df_chunks_train.to_csv('df_chunks_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c8c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd104d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_chunks_train = pd.read_csv('df_chunks_train.csv')\n",
    "# df_chunks_dev = pd.read_csv('df_chunks_dev.csv')\n",
    "# df_chunks_test = pd.read_csv('df_chunks_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217dcac2",
   "metadata": {},
   "source": [
    "# 3. Fine tune a model on labeled chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28152e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer, set_seed \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d887768",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(df_chunks_train[['text', 'label']])\n",
    "ds_dev = Dataset.from_pandas(df_chunks_dev[['text', 'label']])\n",
    "ds_test = Dataset.from_pandas(df_chunks_test[['text', 'label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c00bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5592ae121c42a684e15cbdb9f3eca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/124553 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69586b22cf70449abd8242af4d1f5b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16789 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8c07f466084019af61c0b41ff83e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33690 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=512, padding=\"max_length\", truncation=True)\n",
    "ds_train = ds_train.map(tokenize_function, batched=True, batch_size=512)\n",
    "ds_dev = ds_dev.map(tokenize_function, batched=True, batch_size=512)\n",
    "ds_test = ds_test.map(tokenize_function, batched=True, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c63a663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers.integrations import WandbCallback\n",
    "os.environ[\"WANDB_PROJECT\"]=\"miniLM_politics\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/MiniLM-L12-H384-uncased\", num_labels=2)\n",
    "\n",
    "output_dir = \"/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_miniLM\"   \n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e4d0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_true=labels, y_pred=predictions)\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "    \n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    FPR = FP / (FP + TN) if (FP + TN) > 0 else 0.0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    FNR = FN / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'true_negative_rate': TNR,\n",
    "        'false_positive_rate': FPR,\n",
    "        'false_negative_rate': FNR,\n",
    "        'true_positive_rate': TPR\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34054e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=8,  \n",
    "    weight_decay=0.01,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    seed=seed,\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    logging_dir=output_dir + 'logs/',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    run_name='podcasts-study' + str(seed),\n",
    "    report_to=\"wandb\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280a1131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,    \n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_dev,    \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ea9785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "WandbCallback\n",
      "NotebookProgressCallback\n",
      "/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 2 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbowenyi\u001b[0m (\u001b[33mblablablab-nlp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/wandb/run-20240416_221929-gh6efwkj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/blablablab-nlp/miniLM_politics/runs/gh6efwkj' target=\"_blank\">podcasts-study1</a></strong> to <a href='https://wandb.ai/blablablab-nlp/miniLM_politics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/blablablab-nlp/miniLM_politics' target=\"_blank\">https://wandb.ai/blablablab-nlp/miniLM_politics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/blablablab-nlp/miniLM_politics/runs/gh6efwkj' target=\"_blank\">https://wandb.ai/blablablab-nlp/miniLM_politics/runs/gh6efwkj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1458' max='10384' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1458/10384 18:11 < 1:51:32, 1.33 it/s, Epoch 1.12/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>True Negative Rate</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>True Positive Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.680400</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.504378</td>\n",
       "      <td>0.464440</td>\n",
       "      <td>0.472870</td>\n",
       "      <td>0.456305</td>\n",
       "      <td>0.547174</td>\n",
       "      <td>0.452826</td>\n",
       "      <td>0.543695</td>\n",
       "      <td>0.456305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>0.718138</td>\n",
       "      <td>0.513134</td>\n",
       "      <td>0.410415</td>\n",
       "      <td>0.477589</td>\n",
       "      <td>0.359808</td>\n",
       "      <td>0.649628</td>\n",
       "      <td>0.350372</td>\n",
       "      <td>0.640192</td>\n",
       "      <td>0.359808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 2 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_miniLM/checkpoint-500)... Done. 7.3s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_miniLM/checkpoint-500)... Done. 13.7s\n",
      "/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 2 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_miniLM/checkpoint-1000)... Done. 9.2s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_miniLM/checkpoint-1000)... Done. 13.9s\n",
      "/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "trainer.add_callback(WandbCallback())\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2247bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:33: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 2 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 01:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8330050110816956, 'eval_accuracy': 0.5636390620362125, 'eval_f1': 0.450532610726967, 'eval_precision': 0.39913907284768213, 'eval_recall': 0.5171171171171172, 'eval_true_negative_rate': 0.5882459723167688, 'eval_false_positive_rate': 0.4117540276832312, 'eval_false_negative_rate': 0.4828828828828829, 'eval_true_positive_rate': 0.5171171171171172, 'eval_runtime': 105.1079, 'eval_samples_per_second': 320.528, 'eval_steps_per_second': 1.674, 'epoch': 8.0}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(ds_test)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_result)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39mfinish()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "test_result = trainer.evaluate(ds_test)\n",
    "print(test_result)\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8fb9f",
   "metadata": {},
   "source": [
    "### We noticed that training model on transcript chunk isn't good enough. The highest F-1 score is not even better than chance, which indicates a weak training signal. Considering that we annotate instances at transcript level but train and evaluate at chunk level, the model performance makes sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c79f78",
   "metadata": {},
   "source": [
    "## 4. Train miniLM with Reddit data and transcript chunks\n",
    "- Reddit data are better labeled than transcript chunks. We hope this can make our training signal stronger. \n",
    "- Reddit data source: https://www.kaggle.com/datasets/neelgajare/liberals-vs-conservatives-on-reddit-13000-posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "752207f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red = pd.read_csv('08_reddit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3165dda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12854, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_red.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf3923b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Political Lean</th>\n",
       "      <th>Score</th>\n",
       "      <th>Id</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>URL</th>\n",
       "      <th>Num of Comments</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date Created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No matter who someone is, how they look like, what language they speak, what they wear, remember the human. For the sake of humanity, the working class can and must unite across all arbitrary boundaries.</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>1</td>\n",
       "      <td>t5fybt</td>\n",
       "      <td>socialism</td>\n",
       "      <td>https://v.redd.it/ng5fyl7hp2l81</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.646272e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                         Title  \\\n",
       "0  No matter who someone is, how they look like, what language they speak, what they wear, remember the human. For the sake of humanity, the working class can and must unite across all arbitrary boundaries.   \n",
       "\n",
       "  Political Lean  Score      Id  Subreddit                              URL  \\\n",
       "0        Liberal      1  t5fybt  socialism  https://v.redd.it/ng5fyl7hp2l81   \n",
       "\n",
       "   Num of Comments Text  Date Created  \n",
       "0                0  NaN  1.646272e+09  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_red.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56d8a04",
   "metadata": {},
   "source": [
    "### This is the best external dataset we can find to augment our weakly labeled dataset. We treat Liberal as left wing (0), and Conservative as right wing (1). Although Liberal isn't equal to leftist, we manually examined the dataset and found their definition is pretty close (in this dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "137ba5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Political Lean', 'Score', 'Id', 'Subreddit', 'URL',\n",
       "       'Num of Comments', 'Text', 'Date Created'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_red.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b155af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_text = df_red.Title.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8694dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_label = df_red['Political Lean'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bf9d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_labels = [1 if lean == 'Conservative' else 0 for lean in red_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b56a647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13226c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red = pd.DataFrame({'text':red_text, 'label':red_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "102f5e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No matter who someone is, how they look like, what language they speak, what they wear, remember the human. For the sake of humanity, the working class can and must unite across all arbitrary boundaries.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                          text  \\\n",
       "0  No matter who someone is, how they look like, what language they speak, what they wear, remember the human. For the sake of humanity, the working class can and must unite across all arbitrary boundaries.   \n",
       "\n",
       "   label  \n",
       "0      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_red.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d26b994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12854, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_red.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa236f",
   "metadata": {},
   "source": [
    "## To further balance the impact of weakly labeled chunks, we downsample the dataset of chunks such that it has around the same amount as Reddit Data. Also, we downsampled the number of left-wing chunks to make the class more balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c95f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = pd.concat([df_chunks_train, df_chunks_dev, df_chunks_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9830325c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175032, 14)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26845671",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = df_chunks.sample(frac=0.15, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61e0e17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26255, 14)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d050306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = pd.concat([df_chunks, df_red])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa594de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15905, 14)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks[df_chunks['label']==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ca4a6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23204, 14)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks[df_chunks['label']==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ac7aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left = df_chunks[df_chunks['label']==0].sample(n=16000, random_state=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbe376cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 14)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_left.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "380a8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right = df_chunks[df_chunks['label']==1].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cde3e4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15905, 14)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_right.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b880649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = pd.concat([df_left, df_right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5daea384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31905, 14)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74af893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_dev_test = train_test_split(df_chunks, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e988942",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev, df_test = train_test_split(df_dev_test, test_size=2/3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50cf3d",
   "metadata": {},
   "source": [
    "## 4.1 Finetune a model on downsampled labeled chunks and all Reddit data\n",
    "- We tried a more powerful model: RoBERTa-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f220411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e76b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = Dataset.from_pandas(df_train[['text', 'label']])\n",
    "ds_dev = Dataset.from_pandas(df_dev[['text', 'label']])\n",
    "ds_test = Dataset.from_pandas(df_test[['text', 'label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65b0ecfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417a48bc81a741f6a005c2d6ce18a7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8317da25707042649f82e70ffa0c2ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3190 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf90751c8f74ac9b53978c4bfa666af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6382 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=256, padding=\"max_length\", truncation=True)\n",
    "ds_train = ds_train.map(tokenize_function, batched=True, batch_size=512)\n",
    "ds_dev = ds_dev.map(tokenize_function, batched=True, batch_size=512)\n",
    "ds_test = ds_test.map(tokenize_function, batched=True, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b298cc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(model_id, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76b2d05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbowenyi\u001b[0m (\u001b[33mblablablab-nlp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "from transformers.integrations import WandbCallback\n",
    "os.environ[\"WANDB_PROJECT\"]=\"roberta_base_politics\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "\n",
    "output_dir = \"/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base\"   \n",
    "seed = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06af8d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_true=labels, y_pred=predictions)\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "    \n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    FPR = FP / (FP + TN) if (FP + TN) > 0 else 0.0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    FNR = FN / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'true_negative_rate': TNR,\n",
    "        'false_positive_rate': FPR,\n",
    "        'false_negative_rate': FNR,\n",
    "        'true_positive_rate': TPR\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d180a7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    seed=seed,\n",
    "    save_strategy='steps',\n",
    "    save_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    logging_dir=output_dir + 'logs/',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    run_name='podcasts-study' + str(seed),\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,    \n",
    "    args=training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_dev,    \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ef1bdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are adding a <class 'transformers.integrations.integration_utils.WandbCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "WandbCallback\n",
      "NotebookProgressCallback\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/shared/3/projects/bowenyi/where-is-news/595_Final_Project/wandb/run-20240417_032414-xji30f3k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/blablablab-nlp/roberta_base_politics/runs/xji30f3k' target=\"_blank\">podcasts-study1</a></strong> to <a href='https://wandb.ai/blablablab-nlp/roberta_base_politics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/blablablab-nlp/roberta_base_politics' target=\"_blank\">https://wandb.ai/blablablab-nlp/roberta_base_politics</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/blablablab-nlp/roberta_base_politics/runs/xji30f3k' target=\"_blank\">https://wandb.ai/blablablab-nlp/roberta_base_politics/runs/xji30f3k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1047' max='1047' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1047/1047 27:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>True Negative Rate</th>\n",
       "      <th>False Positive Rate</th>\n",
       "      <th>False Negative Rate</th>\n",
       "      <th>True Positive Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.685500</td>\n",
       "      <td>0.664602</td>\n",
       "      <td>0.589342</td>\n",
       "      <td>0.539705</td>\n",
       "      <td>0.593050</td>\n",
       "      <td>0.495164</td>\n",
       "      <td>0.678462</td>\n",
       "      <td>0.321538</td>\n",
       "      <td>0.504836</td>\n",
       "      <td>0.495164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.662900</td>\n",
       "      <td>0.646654</td>\n",
       "      <td>0.608777</td>\n",
       "      <td>0.634232</td>\n",
       "      <td>0.581408</td>\n",
       "      <td>0.697614</td>\n",
       "      <td>0.524710</td>\n",
       "      <td>0.475290</td>\n",
       "      <td>0.302386</td>\n",
       "      <td>0.697614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.643300</td>\n",
       "      <td>0.647983</td>\n",
       "      <td>0.616301</td>\n",
       "      <td>0.607441</td>\n",
       "      <td>0.604340</td>\n",
       "      <td>0.610574</td>\n",
       "      <td>0.621721</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.389426</td>\n",
       "      <td>0.610574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.629300</td>\n",
       "      <td>0.633642</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.659155</td>\n",
       "      <td>0.585293</td>\n",
       "      <td>0.754352</td>\n",
       "      <td>0.494204</td>\n",
       "      <td>0.505796</td>\n",
       "      <td>0.245648</td>\n",
       "      <td>0.754352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.602800</td>\n",
       "      <td>0.645515</td>\n",
       "      <td>0.638871</td>\n",
       "      <td>0.577713</td>\n",
       "      <td>0.669499</td>\n",
       "      <td>0.508059</td>\n",
       "      <td>0.762660</td>\n",
       "      <td>0.237340</td>\n",
       "      <td>0.491941</td>\n",
       "      <td>0.508059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.599800</td>\n",
       "      <td>0.629489</td>\n",
       "      <td>0.637304</td>\n",
       "      <td>0.649288</td>\n",
       "      <td>0.612700</td>\n",
       "      <td>0.690522</td>\n",
       "      <td>0.586943</td>\n",
       "      <td>0.413057</td>\n",
       "      <td>0.309478</td>\n",
       "      <td>0.690522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.589800</td>\n",
       "      <td>0.635375</td>\n",
       "      <td>0.638245</td>\n",
       "      <td>0.637563</td>\n",
       "      <td>0.621555</td>\n",
       "      <td>0.654417</td>\n",
       "      <td>0.622941</td>\n",
       "      <td>0.377059</td>\n",
       "      <td>0.345583</td>\n",
       "      <td>0.654417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.553200</td>\n",
       "      <td>0.651482</td>\n",
       "      <td>0.638245</td>\n",
       "      <td>0.612492</td>\n",
       "      <td>0.639103</td>\n",
       "      <td>0.588008</td>\n",
       "      <td>0.685784</td>\n",
       "      <td>0.314216</td>\n",
       "      <td>0.411992</td>\n",
       "      <td>0.588008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.550400</td>\n",
       "      <td>0.646328</td>\n",
       "      <td>0.641379</td>\n",
       "      <td>0.646259</td>\n",
       "      <td>0.620915</td>\n",
       "      <td>0.673759</td>\n",
       "      <td>0.610738</td>\n",
       "      <td>0.389262</td>\n",
       "      <td>0.326241</td>\n",
       "      <td>0.673759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.548300</td>\n",
       "      <td>0.649646</td>\n",
       "      <td>0.647649</td>\n",
       "      <td>0.637887</td>\n",
       "      <td>0.637476</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.656498</td>\n",
       "      <td>0.343502</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.638298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-100)... Done. 28.8s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-100)... Done. 53.2s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-200)... Done. 27.8s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-200)... Done. 54.9s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-300)... Done. 28.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-300)... Done. 58.8s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-400)... Done. 29.2s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-400)... Done. 56.5s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-500)... Done. 33.0s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-500)... Done. 53.1s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-600)... Done. 31.1s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-600)... Done. 54.1s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-700)... Done. 27.5s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-700)... Done. 55.8s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-800)... Done. 31.4s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-800)... Done. 53.3s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-900)... Done. 31.7s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-900)... Done. 53.5s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-1000)... Done. 28.1s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/shared/3/projects/bowenyi/where-is-news/595_486_Final_Project/model_output_roberta_base/checkpoint-1000)... Done. 53.6s\n",
      "/opt/anaconda/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='15256.371 MB of 15256.371 MB uploaded (32.141 MB deduped)\\r'), FloatProgress(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▃▃▄▄▅▅▇▇▇▇▇▇▇▇▇▇██▄▄</td></tr><tr><td>eval/f1</td><td>▁▁▇▇▅▅██▃▃▇▇▇▇▅▅▇▇▇▇██</td></tr><tr><td>eval/false_negative_rate</td><td>██▃▃▅▅▁▁██▃▃▄▄▅▅▃▃▄▄▁▁</td></tr><tr><td>eval/false_positive_rate</td><td>▃▃▇▇▅▅██▁▁▅▅▅▅▃▃▅▅▄▄██</td></tr><tr><td>eval/loss</td><td>██▄▄▅▅▂▂▄▄▁▁▂▂▅▅▄▄▅▅▂▂</td></tr><tr><td>eval/precision</td><td>▂▂▁▁▃▃▂▂██▄▄▄▄▆▆▄▄▆▆▁▁</td></tr><tr><td>eval/recall</td><td>▁▁▆▆▄▄██▁▁▆▆▅▅▄▄▆▆▅▅██</td></tr><tr><td>eval/runtime</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██</td></tr><tr><td>eval/samples_per_second</td><td>▂▂▂▂▁▁▃▃▅▅▃▃▃▃▅▅▄▄▄▄██</td></tr><tr><td>eval/steps_per_second</td><td>▂▂▂▂▁▁▃▃▅▅▃▃▃▃▅▅▄▄▄▄██</td></tr><tr><td>eval/true_negative_rate</td><td>▆▆▂▂▄▄▁▁██▄▄▄▄▆▆▄▄▅▅▁▁</td></tr><tr><td>eval/true_positive_rate</td><td>▁▁▆▆▄▄██▁▁▆▆▅▅▄▄▆▆▅▅██</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇███████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇███████</td></tr><tr><td>train/grad_norm</td><td>▁▁▃▃▃▃▃▃▄▄▇▇▄▄██▇▇██</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▆▆▅▅▄▄▃▃▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▆▆▅▅▄▄▄▄▃▃▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.61109</td></tr><tr><td>eval/f1</td><td>0.65082</td></tr><tr><td>eval/false_negative_rate</td><td>0.25435</td></tr><tr><td>eval/false_positive_rate</td><td>0.51616</td></tr><tr><td>eval/loss</td><td>0.63575</td></tr><tr><td>eval/precision</td><td>0.57738</td></tr><tr><td>eval/recall</td><td>0.74565</td></tr><tr><td>eval/runtime</td><td>23.2426</td></tr><tr><td>eval/samples_per_second</td><td>274.582</td></tr><tr><td>eval/steps_per_second</td><td>4.302</td></tr><tr><td>eval/true_negative_rate</td><td>0.48384</td></tr><tr><td>eval/true_positive_rate</td><td>0.74565</td></tr><tr><td>total_flos</td><td>8814088799032320.0</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>1047</td></tr><tr><td>train/grad_norm</td><td>6.61168</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5483</td></tr><tr><td>train_loss</td><td>0.6039</td></tr><tr><td>train_runtime</td><td>1680.4768</td></tr><tr><td>train_samples_per_second</td><td>39.869</td></tr><tr><td>train_steps_per_second</td><td>0.623</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">podcasts-study1</strong> at: <a href='https://wandb.ai/blablablab-nlp/roberta_base_politics/runs/xji30f3k' target=\"_blank\">https://wandb.ai/blablablab-nlp/roberta_base_politics/runs/xji30f3k</a><br/>Synced 6 W&B file(s), 0 media file(s), 136 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240417_032414-xji30f3k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.add_callback(WandbCallback())\n",
    "trainer.train()\n",
    "trainer.evaluate(ds_test)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997740a4",
   "metadata": {},
   "source": [
    "## This strategy achieved a better F-1 score. Our next step is to manually check 1000 samples predicted by RoBERTa. Then, we can set a proper decision threshold for right wing and left wing, instead of using the default 0.5. \n",
    "\n",
    "## We should have used a separate set of data for calibration. But due to high-quality data scarcity, we used the test set to calibrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e883a48",
   "metadata": {},
   "source": [
    "## We divided the entire test set to 10 ten-percent bins. Then, we randomly sampled 100 instanced in each 10 percent bin for manually checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50953876",
   "metadata": {},
   "source": [
    "## 4.2 Produce file for calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4aa80c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.special import softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "286dfd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = softmax(predictions.predictions, axis=1)[:, 1]\n",
    "df_test['prob'] = probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55bf9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['stratum'] = pd.cut(df_test['prob'], bins=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], \n",
    "                       labels=['0-10%', '10-20%', '20-30%', '30-40%', '40-50%', '50-60%', '60-70%', '70-80%', '80-90%', '90-100%'], \n",
    "                       include_lowest=True)\n",
    "\n",
    "df_test['ground_truth'] = ''\n",
    "df_test['note'] = ''\n",
    "df_test_cp = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ea3b7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_df = df_test_cp.groupby('stratum').apply(lambda x: x.sample(n=100, random_state=1)).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f6d2c187",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_df.to_csv('09_RoBERTa_annotate.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be09bc",
   "metadata": {},
   "source": [
    "## After manually checking, we decide 0.7 would be a decision threshold. If an input text receives score above 0.7, it's right-wing. If lower, it's left-wing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e7d3c",
   "metadata": {},
   "source": [
    "## 4.3 Out of curiosity, we tested a finetuned BERT-based model specifically for detecting political leaning. The model is introduced by an EMNLP article in 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13cb8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "pol_bert = AutoModelForSequenceClassification.from_pretrained(\"bucketresearch/politicalBiasBERT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b65964b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts = [entry['text'] for entry in ds_test]  \n",
    "tokenized_texts = tokenizer(texts, max_length=256, padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_texts_dataset = Dataset.from_dict(tokenized_texts)\n",
    "\n",
    "polbert_trainer = Trainer(model=model)\n",
    "\n",
    "polbert_predictions = polbert_trainer.predict(tokenized_texts_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4e9dcaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b52be99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6508159819921215\n",
      "Accuracy: 0.6110937010341586\n"
     ]
    }
   ],
   "source": [
    "polbert_probs = torch.nn.functional.softmax(torch.tensor(polbert_predictions.predictions), dim=-1).numpy()\n",
    "\n",
    "predicted_labels = np.argmax(polbert_probs, axis=1)\n",
    "\n",
    "true_labels = np.array([entry['label'] for entry in ds_test])  # Ensure the key 'label' matches the key in ds_test\n",
    "\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b69e4b",
   "metadata": {},
   "source": [
    "## Seems that the pretrained model didn't perform better than our own finetuned model. (Also to show effort), we'll use our own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0856f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir = \"best_roberta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058adfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
